{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN9LFsEMpSA+c6uRvl+rHYK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Preprocessing"],"metadata":{"id":"SberG0jGWnrE"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1vT1ARI6R6zQ","executionInfo":{"status":"ok","timestamp":1759315232356,"user_tz":-330,"elapsed":42078,"user":{"displayName":"Mena Rossini R","userId":"03024307458201324126"}},"outputId":"07cee1e5-2b88-4b07-9230-88fd2b1f2792"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2623732386.py:5: DtypeWarning: Columns (1,3,39,47) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv('joined_dataset.csv')\n"]},{"output_type":"stream","name":"stdout","text":["Categorical Columns (to be encoded): Index(['Source IP', 'Source Port', 'Destination IP', 'Destination Port',\n","       'Protocol', 'Connection State', 'Service', 'FTP Command Count',\n","       'Attack Category'],\n","      dtype='object')\n","\n","Missing values per column:\n"," Source IP                                                                     0\n","Source Port                                                                   0\n","Destination IP                                                                0\n","Destination Port                                                              0\n","Protocol                                                                      0\n","Connection State                                                              0\n","Duration                                                                      0\n","Source Bytes                                                                  0\n","Destination Bytes                                                             0\n","Source TTL                                                                    0\n","Destination TTL                                                               0\n","Source Packet Loss                                                            0\n","Destination Packet Loss                                                       0\n","Service                                                                       0\n","Source Load                                                                   0\n","Destination Load                                                              0\n","Source Packets                                                                0\n","Destination Packets                                                           0\n","Source Window Size                                                            0\n","Destination Window Size                                                       0\n","Source TCP Base Seq                                                           0\n","Destination TCP Base Seq                                                      0\n","Source Mean Packet Size                                                       0\n","Destination Mean Packet Size                                                  0\n","Transaction Depth                                                             0\n","Response Body Length                                                          0\n","Source Jitter                                                                 0\n","Destination Jitter                                                            0\n","Start Time                                                                    0\n","End Time                                                                      0\n","Source Inter-Packet Arrival Time                                              0\n","Destination Inter-Packet Arrival Time                                         0\n","TCP Round Trip Time                                                           0\n","SYN-ACK Time                                                                  0\n","ACK Data Time                                                                 0\n","Same IPs/Ports Flag                                                           0\n","Count State TTL                                                               0\n","HTTP Method Count                                                       1348145\n","FTP Login Flag                                                          1429879\n","FTP Command Count                                                             0\n","Connections from Same Source to Same Service                                  0\n","Connections to Same Destination Service                                       0\n","Connections to Same Destination in Last Minute                                0\n","Connections from Same Source in Last Minute                                   0\n","Connections from Same Source to Same Destination Port in Last Minute          0\n","Connections from Same Destination Source Port in Last Minute                  0\n","Connections between Same Destination and Source in Last Minute                0\n","Attack Category                                                         2218764\n","Label                                                                         0\n","dtype: int64\n","\n","Remaining non-finite values (should be 0): 0\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","# Assuming your merged dataframe is loaded as 'df'\n","df = pd.read_csv('joined_dataset.csv')\n","\n","# --- 1. Identify Categorical Columns (Objects) ---\n","# Find columns with 'object' datatype (usually strings/text)\n","categorical_cols = df.select_dtypes(include=['object']).columns\n","print(\"Categorical Columns (to be encoded):\", categorical_cols)\n","\n","# --- 2. Handle NaN/Null Values ---\n","# Check for null values in each column\n","print(\"\\nMissing values per column:\\n\", df.isnull().sum())\n","\n","# A common, simple approach is to fill missing values with 0.\n","# For a complex dataset like this, filling with 0 or the median is better than dropping rows.\n","# Replace all NaN values with 0.\n","df = df.fillna(0)\n","\n","\n","# --- 3. Handle Infinite Values (inf) ---\n","# In network datasets, division by zero can create 'inf' values (e.g., in load, jitter, or rates).\n","# We must replace these with a sensible number, like the maximum finite value in that column.\n","\n","# Replace all 'inf' and '-inf' values with NaN first\n","df = df.replace([np.inf, -np.inf], np.nan)\n","\n","# Now, fill the new NaN values (which were previously inf) with a large finite number.\n","# Using a fixed large number like 1e9 is safer than using the column mean/median for inf.\n","df = df.fillna(1e9) # Fill with 1 billion, ensuring it is a large finite number\n","\n","# Re-check for any remaining non-finite values (should be 0)\n","print(\"\\nRemaining non-finite values (should be 0):\", df.isin([np.inf, -np.inf, np.nan]).sum().sum())"]},{"cell_type":"markdown","source":["ENCODING -  we use one hot encoding (even though it may increase the size than binary encoding)\n","\n","OHE is better as it doesnt support ordinality\n","like if a no 1 is assigned to tcp, and 3 is for ip...it wont think ip is superior than tcp.\n","\n","and we dont want ordinality when it comes to FE and NN"],"metadata":{"id":"yiRDw13TW_RM"}},{"cell_type":"code","source":["# --- 4. One-Hot Encode Categorical Features ---\n","\n","# These are the columns that need encoding:\n","# ['Protocol', 'Connection State', 'Service', 'Attack Category']\n","# NOTE: 'Label' is already 0/1, so it does not need encoding.\n","\n","cols_to_encode = ['Protocol', 'Connection State', 'Service', 'Attack Category']\n","\n","# Use pandas get_dummies for One-Hot Encoding\n","df = pd.get_dummies(df, columns=cols_to_encode, prefix=cols_to_encode)\n","\n","print(\"\\nDataFrame Shape after One-Hot Encoding:\", df.shape)\n","print(\"New Columns (a sample):\", df.columns[-10:].tolist())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VpK7e4R4XvYL","executionInfo":{"status":"ok","timestamp":1759315435716,"user_tz":-330,"elapsed":2256,"user":{"displayName":"Mena Rossini R","userId":"03024307458201324126"}},"outputId":"02b08ad5-eaca-4a26-812d-ec913a4b0f9a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","DataFrame Shape after One-Hot Encoding: (2540047, 223)\n","New Columns (a sample): ['Attack Category_ Shellcode ', 'Attack Category_Analysis', 'Attack Category_Backdoor', 'Attack Category_Backdoors', 'Attack Category_DoS', 'Attack Category_Exploits', 'Attack Category_Generic', 'Attack Category_Reconnaissance', 'Attack Category_Shellcode', 'Attack Category_Worms']\n"]}]},{"cell_type":"markdown","source":["Save the dataset as a save point"],"metadata":{"id":"grVI9_8tgMet"}},{"cell_type":"code","source":["# Save the cleaned and encoded dataframe for a checkpoint\n","df.to_csv(\"checkpoint_preprocessed_unsw.csv\", index=False)\n","print(\"Saved checkpoint_preprocessed_unsw.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5d4ile9KgMKc","executionInfo":{"status":"ok","timestamp":1759317907684,"user_tz":-330,"elapsed":247841,"user":{"displayName":"Mena Rossini R","userId":"03024307458201324126"}},"outputId":"db7502a6-d5f6-42ea-c64e-769c97d7199b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved checkpoint_preprocessed_unsw.csv\n"]}]},{"cell_type":"markdown","source":["Feature selection and stratifies sampling - as i dont enough power"],"metadata":{"id":"p_mxrLFUje9X"}},{"cell_type":"code","source":["# --- 1. Identify and Drop ID and Redundant Columns ---\n","# 'Source IP', 'Destination IP', 'Source Port', 'Destination Port' have too many unique values (high cardinality).\n","# The original Attack Category column has been replaced by the One-Hot encoded columns.\n","# We also drop other unique identifiers like TCP Base Seq and time stamps.\n","\n","cols_to_drop = [\n","    'Source IP', 'Destination IP', 'Source Port', 'Destination Port',\n","    'Source TCP Base Seq', 'Destination TCP Base Seq', 'Start Time', 'End Time'\n","]\n","\n","# Drop the columns from the dataframe\n","df = df.drop(columns=cols_to_drop, errors='ignore')\n","\n","# Separate the target variable 'Label' (0 or 1)\n","X = df.drop('Label', axis=1) # Features\n","y = df['Label']             # Target"],"metadata":{"id":"g18TonQCjk82","executionInfo":{"status":"ok","timestamp":1759318561982,"user_tz":-330,"elapsed":3069,"user":{"displayName":"Mena Rossini R","userId":"03024307458201324126"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Separate features (X) and target (y) again, after dropping the ID columns (Action 2.1)\n","# NOTE: We assume X and y were successfully created in Action 2.1\n","# Let's verify the size of the whole dataset first:\n","print(f\"Original Dataset Size: {df.shape[0]} rows\")\n","\n","# We will sample 500,000 rows. Use train_test_split with stratify=y to ensure the\n","# sample maintains the same attack/normal ratio as the full dataset.\n","sample_size = 500000\n","\n","# We use the test_size to determine the percentage needed for the sample_size\n","# sample_fraction = sample_size / len(df)\n","\n","# Perform stratified sampling to get the final working dataset\n","X_sample, _, y_sample, _ = train_test_split(\n","    X, y,\n","    test_size=(1 - sample_size / len(df)),\n","    random_state=42, # Ensure reproducibility\n","    stratify=y       # CRITICAL: Preserves the ratio of 0s and 1s\n",")\n","\n","# Recombine the sample for scaling\n","df_sampled = X_sample.copy()\n","df_sampled['Label'] = y_sample\n","\n","print(f\"Sampled Dataset Size: {df_sampled.shape[0]} rows\")\n","print(\"New Sampled Label Distribution:\\n\", df_sampled['Label'].value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M7zAd8CmjOUS","executionInfo":{"status":"ok","timestamp":1759318656297,"user_tz":-330,"elapsed":10338,"user":{"displayName":"Mena Rossini R","userId":"03024307458201324126"}},"outputId":"293b5ced-b57c-4b46-c514-6abc5f9eb13b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Dataset Size: 2540047 rows\n","Sampled Dataset Size: 500000 rows\n","New Sampled Label Distribution:\n"," Label\n","0    436756\n","1     63244\n","Name: count, dtype: int64\n"]}]},{"cell_type":"markdown","source":["FEature Scaling - We will use MinMaxScaler to scale all continuous features to the range [0,1]."],"metadata":{"id":"QycqIXRRkHLy"}},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler\n","\n","# --- 1. Identify Numerical Features for Scaling ---\n","# We exclude the target 'Label' and all one-hot encoded (OHE) columns (which are already 0 or 1)\n","\n","# Get column names that are NOT the Label and are NOT part of the OHE binary columns\n","# OHE columns end in things like '_tcp', '_http', etc., but for safety, we identify all non-OHE columns.\n","# A quick way is to select all columns that are NOT of type 'uint8' (typical OHE type)\n","# or are not the target 'Label'.\n","\n","# Assuming your OHE columns are the only 'uint8' type:\n","cols_to_scale = df_sampled.select_dtypes(include=['int64', 'float64']).columns.drop('Label', errors='ignore')\n","\n","print(\"Columns to Scale:\", cols_to_scale.tolist())\n","\n","# --- 2. Apply MinMaxScaler ---\n","scaler = MinMaxScaler()\n","\n","# Apply scaler only to the selected continuous columns\n","df_sampled[cols_to_scale] = scaler.fit_transform(df_sampled[cols_to_scale])\n","\n","# Now all features are clean, scaled, and numerical!"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3StnHyQvkKP1","executionInfo":{"status":"ok","timestamp":1759318702794,"user_tz":-330,"elapsed":619,"user":{"displayName":"Mena Rossini R","userId":"03024307458201324126"}},"outputId":"e2928887-f748-45a1-c346-cc7a6a66e928"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Columns to Scale: ['Duration', 'Source Bytes', 'Destination Bytes', 'Source TTL', 'Destination TTL', 'Source Packet Loss', 'Destination Packet Loss', 'Source Load', 'Destination Load', 'Source Packets', 'Destination Packets', 'Source Window Size', 'Destination Window Size', 'Source Mean Packet Size', 'Destination Mean Packet Size', 'Transaction Depth', 'Response Body Length', 'Source Jitter', 'Destination Jitter', 'Source Inter-Packet Arrival Time', 'Destination Inter-Packet Arrival Time', 'TCP Round Trip Time', 'SYN-ACK Time', 'ACK Data Time', 'Same IPs/Ports Flag', 'Count State TTL', 'HTTP Method Count', 'FTP Login Flag', 'Connections from Same Source to Same Service', 'Connections to Same Destination Service', 'Connections to Same Destination in Last Minute', 'Connections from Same Source in Last Minute', 'Connections from Same Source to Same Destination Port in Last Minute', 'Connections from Same Destination Source Port in Last Minute', 'Connections between Same Destination and Source in Last Minute']\n"]}]},{"cell_type":"code","source":["# Assuming 'df_sampled' is your final, clean, scaled, and downsampled DataFrame\n","df_sampled.to_csv(\"final_golden_dataset_500k.csv\", index=False)\n","print(\"Dataset saved as final_golden_dataset_500k.csv. This is your definitive starting point for all model training.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fKpYImh4luzY","executionInfo":{"status":"ok","timestamp":1759319173316,"user_tz":-330,"elapsed":68514,"user":{"displayName":"Mena Rossini R","userId":"03024307458201324126"}},"outputId":"8f852ba7-0a18-4dfc-a2d7-80f97e106e99"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset saved as final_golden_dataset_500k.csv. This is your definitive starting point for all model training.\n"]}]}]}