{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNjjaPj5JnUBr1VZB3fyafI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["XG BOOST"],"metadata":{"id":"jegyzj6E2PIq"}},{"cell_type":"markdown","source":["Our data is an imbalances data (more entry on normal than the attack)\n","\n","XGBoost (Extreme Gradient Boosting) is a good choice cuz-\n","\n","  Imbalance Handling: It naturally supports the scale_pos_weight parameter. This parameter tells the algorithm to weigh misclassifications of the rare class (Attacks) much higher, forcing the model to pay more attention to the anomalies.\n","\n","  Ensemble Power: XGBoost is an ensemble model built on Decision Trees. It learns sequentially: each new tree corrects the errors (residuals) of the previous tree. This incremental learning is highly effective at capturing the complex, subtle patterns that define an anomaly, outperforming simpler models like Logistic Regression.\n","\n","  Its like smaller decision trees are made and put together."],"metadata":{"id":"CL_Nlsq32u5M"}},{"cell_type":"markdown","source":["Importing necessary things"],"metadata":{"id":"Viuskxlz2Sve"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from xgboost import XGBClassifier\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","import numpy as np\n","\n","# Load the saved, preprocessed, and scaled dataset\n","df_sampled = pd.read_csv(\"Dataset.csv\")\n","\n","# -------------------------------------------------------------\n","\n","'''\n","so when i run the XG BOOST normally - it got a data leakage issue cus of  the column FTP Command Count  as it is still an object (string) type and XG wants numeric data\n","\n","so to fix the issue and convert problematic 'object' column to numeric\n","\n","1) We use errors='coerce' to turn any remaining non-numeric text (like ' - ') into NaN,\n","2) then we fill those NaNs with 0 (or the column median, but 0 is safe here).\n","\n","'''\n","\n","# FIX:\n","df_sampled['FTP Command Count'] = pd.to_numeric(\n","    df_sampled['FTP Command Count'],\n","    errors='coerce' # Convert non-numbers to NaN\n",")\n","df_sampled['FTP Command Count'] = df_sampled['FTP Command Count'].fillna(0)\n","\n","# Verification (Check to ensure no other 'object' types remain)\n","object_cols = df_sampled.select_dtypes(include=['object']).columns\n","if len(object_cols) > 0:\n","    print(f\"WARNING: Still found object columns: {object_cols.tolist()}\")\n","else:\n","    print(\"SUCCESS: All feature columns are now numerical.\")\n","\n","# Separate Features (X) and Target (y)\n","X = df_sampled.drop('Label', axis=1)\n","y = df_sampled['Label']\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ufNNPGIF4d1Z","executionInfo":{"status":"ok","timestamp":1759324315499,"user_tz":-330,"elapsed":21843,"user":{"displayName":"Mena Rossini R","userId":"03024307458201324126"}},"outputId":"a9aa3325-9a97-49fd-b7c7-448456220aff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SUCCESS: All feature columns are now numerical.\n"]}]},{"cell_type":"markdown","source":["SO i faced a leaky data problem - where the model memorised the outcome by some clues (leaky data) ...  \n","\n","so i added the below code\n","\n"],"metadata":{"id":"7QFo3w1J7lHv"}},{"cell_type":"code","source":["# Assuming X and y are loaded from Dataset.csv\n","\n","# -------------------------------------------------------------\n","# ðŸš¨ CRITICAL FIX: Remove features that leak the target (Label)\n","# -------------------------------------------------------------\n","\n","# 1. Identify all OHE columns created from 'Attack Category'\n","leaky_cols = [col for col in X.columns if 'Attack Category' in col]\n","\n","# 2. Also drop 'FTP Command Count' as a precaution due to its initial 'object' type and potential leakage\n","leaky_cols.append('FTP Command Count')\n","\n","# Remove the leaky columns from the features X\n","X = X.drop(columns=leaky_cols, errors='ignore')\n","\n","print(f\"Dropped {len(leaky_cols)} leaky features.\")\n","print(f\"New Feature Count: {X.shape[1]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qo-MlAkK7xtu","executionInfo":{"status":"ok","timestamp":1759324896100,"user_tz":-330,"elapsed":220,"user":{"displayName":"Mena Rossini R","userId":"03024307458201324126"}},"outputId":"5706181f-27d4-48e4-82b6-8ca80d01dc88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dropped 15 leaky features.\n","New Feature Count: 199\n"]}]},{"cell_type":"markdown","source":["Split the data"],"metadata":{"id":"9iDrRsfl2Vr9"}},{"cell_type":"code","source":["# Split 70% for Training, 30% for Testing\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y,\n","    test_size=0.3,\n","    random_state=42,\n","    stratify=y     # CRITICAL: Preserves the Attack/Normal ratio\n",")\n","\n","print(f\"Training Set Size: {X_train.shape[0]} rows\")\n","print(f\"Testing Set Size: {X_test.shape[0]} rows\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hLe25Ndd2Y0Q","executionInfo":{"status":"ok","timestamp":1759324901632,"user_tz":-330,"elapsed":733,"user":{"displayName":"Mena Rossini R","userId":"03024307458201324126"}},"outputId":"734c27e4-5d02-498f-f64a-e84b2ad2b024"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Set Size: 350000 rows\n","Testing Set Size: 150000 rows\n"]}]},{"cell_type":"markdown","source":["Train XGBoost (Handling Imbalance)\n","\n","We use the scale_pos_weight parameter to address the class imbalance, which gives more importance to the rare Attack class (1)."],"metadata":{"id":"J1-OOsBp2blQ"}},{"cell_type":"code","source":["# Calculate the imbalance ratio (Count of Normal / Count of Attack)\n","ratio = y_train.value_counts()[0] / y_train.value_counts()[1]\n","print(f\"Class Imbalance Ratio (0/1): {ratio:.2f}\")\n","\n","# Initialize and train the XGBoost model\n","xgb_model = XGBClassifier(\n","    objective='binary:logistic',\n","    n_estimators=100,\n","    learning_rate=0.1,\n","    scale_pos_weight=ratio, # Applying the weight to balance classes\n","    use_label_encoder=False,\n","    eval_metric='logloss',\n","    random_state=42\n",")\n","\n","print(\"Starting XGBoost training...\")\n","xgb_model.fit(X_train, y_train)\n","print(\"XGBoost training complete.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YRHtMEzX2hqm","executionInfo":{"status":"ok","timestamp":1759324924766,"user_tz":-330,"elapsed":20589,"user":{"displayName":"Mena Rossini R","userId":"03024307458201324126"}},"outputId":"b93201ef-81d1-4aa3-e5b8-c6b53204fa4b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Class Imbalance Ratio (0/1): 6.91\n","Starting XGBoost training...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [13:21:50] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost training complete.\n"]}]},{"cell_type":"markdown","source":["Results"],"metadata":{"id":"v554Qq3p2kri"}},{"cell_type":"code","source":["# Predict on the held-out TEST data\n","y_pred = xgb_model.predict(X_test)\n","y_proba = xgb_model.predict_proba(X_test)[:, 1]\n","\n","# Calculate Metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","roc_auc = roc_auc_score(y_test, y_proba)\n","\n","# Print the Centralized Baseline Results for your paper\n","print(\"\\n--- XGBoost Centralized Baseline Results ---\")\n","print(f\"Accuracy:  {accuracy:.4f}\")\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall:    {recall:.4f}\")\n","print(f\"F1 Score:  {f1:.4f}\")\n","print(f\"ROC-AUC:   {roc_auc:.4f} (Benchmark Metric!)\")\n","\n","# You should save these 5 numbers as your first benchmark!"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iiO8uYU42kS0","executionInfo":{"status":"ok","timestamp":1759324936847,"user_tz":-330,"elapsed":613,"user":{"displayName":"Mena Rossini R","userId":"03024307458201324126"}},"outputId":"c94b6f16-75b8-471a-de75-c3fba4975002"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- XGBoost Centralized Baseline Results ---\n","Accuracy:  0.9881\n","Precision: 0.9141\n","Recall:    0.9999\n","F1 Score:  0.9551\n","ROC-AUC:   0.9997 (Benchmark Metric!)\n"]}]}]}